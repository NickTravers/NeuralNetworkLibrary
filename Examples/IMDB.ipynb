{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Data Source}$: \n",
    "Large Movie Review Dataset v1.0 (IMDB movie reviews).\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "$\\textbf{Data Preparation}$: To run this Jupyter Notebook, download the dataset from the above website. Then place the 'train' and 'test' folders into the 'IMDB' directory of your cloned version of the NeuralNetworkLibrary repository.\n",
    "\n",
    "$\\textbf{Objective}$:\n",
    "Text Classification - Predict label 'pos' or 'neg' for each review, based on text of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from Applications.Text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews, neg_reviews = os.listdir('IMDB/train/pos'), os.listdir('IMDB/train/neg')\n",
    "num_pos, num_neg = len(pos_reviews), len(neg_reviews)\n",
    "\n",
    "fpos = open('IMDB/train/pos/'+pos_reviews[0],'r')\n",
    "pos = fpos.read()\n",
    "\n",
    "fneg = open('IMDB/train/neg/'+neg_reviews[0],'r')\n",
    "neg = fneg.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Reviews: 12500 \n",
      "\n",
      "Number of Negative Reviews: 12500 \n",
      "\n",
      "Sample Positive Review: \n",
      " Fantastic documentary of 1924. This early 20th century geography of today's Iraq was powerful. Watch this and tell me if Cecil B. DeMille didn't take notes before making his The Ten Commandments. Merian C. Cooper, the photographer, later created Cinerama, an idea that probably hatched while filming the remarkable landscapes in this film. Fans of Werner Herzog will find this film to be a treasure, with heartbreaking tales of struggle, complimented by the land around them, never has the human capacity to endure been so evident. The fact that this was made when it was shows not only the will of the subjects, but of the filmmakers themselves. \n",
      "\n",
      "Sample Negative Review: \n",
      " Basically, Cruel Intentions 2 is Cruel Intentions 1, again, only poorly done. The story is exactly the same as the first one (even some of the lines), with only a few exceptions. The cast is more unknown, and definitely less talented. Instead of being seductive and drawing me into watching it, I ended up feeling dirty because it compares to watching a soft-core porn. I'm not sure whether to blame some of the idiotic lines on the actors or the writers...and I always feel bad saying that, because I know how hard it is to do both...but it was basically a two-hour waste of my life. It literally amazes me that some movies get made, and this is no exception...I can't believe they'd make a third one.\n"
     ]
    }
   ],
   "source": [
    "print('Number of Positive Reviews:', num_pos, '\\n')\n",
    "print('Number of Negative Reviews:', num_neg, '\\n')\n",
    "\n",
    "print('Sample Positive Review: \\n', pos, '\\n')\n",
    "print('Sample Negative Review: \\n', neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Step 1:}$\n",
    "Build a language model for the IMDB review corpus, using transfer learning from a model pretrained on the wikitext103 corpus. \n",
    "\n",
    "$\\textbf{Step 2:}$\n",
    "Build a text classifier for the IMDB reviews. This uses the same LSTM encoder architecture from the language model, along with an attention-based decoder to combine encoder outputs. The text classifier encoder is initialized with the weights from the trained language model, but finetuned in training.\n",
    "\n",
    "** Additionally, we train with the texts in both the forward and backward directions, and average the predictions, to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (1) - Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We use the unsupported reviews without labels, as well as the labeled 'pos' and 'neg' reviews, to create the language model. We expect the text of the unlabeled reviews to be similar to that of the labeled ones, and the additional data (50,000 additional reviews) should help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ...\n",
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513aea5f61334838b75e271a6a03212a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06761a7ed85f4fe7a8a02d1488c243d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f8d210667c4e9298fe92d49634e539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2693edee95d84a32811ec71dde4fd1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ef54b68b91406aa102c0edb59b8e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5e0e7e23ed42399d6d0cde99f52fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Numericalizing ...\n",
      "min freq = 6 , max_vocab = 60000\n",
      "Done, vocab_size =  47343\n"
     ]
    }
   ],
   "source": [
    "data = LanguageModelDataObj.from_folders(bs=64, bptt=75, labels=['pos','neg','unsup'], \n",
    "                                         train='IMDB/train', reverse=False)\n",
    "\n",
    "# save string-to-int dictionary from data\n",
    "pickle.dump(data.stoi, open('IMDB/data_stoi_fwd','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'IMDB'\n",
    "model = LanguageModelNet(data, pretrained='fwd')\n",
    "model.clear_non_raw()\n",
    "opt_func = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "optimizer = Optimizer(opt_func, model)\n",
    "loss_func = RegSeqCrossEntropyLoss()\n",
    "learner = Learner(PATH,data,model,optimizer,loss_func)\n",
    "learner.freeze()\n",
    "\n",
    "# metrics\n",
    "acc = LanguageModelAccuracy()\n",
    "ce = SeqCrossEntropyLoss(loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The loss function RegSeqCrossEntropyLoss is the standard cross entropy loss between predicted and actual \n",
    "tokens + a regularization term applied to the output of the LSTM encoder. The metric SeqCrossEntropyLoss gives \n",
    "the unregularized cross entropy loss. The metric LanguageModelAccuracy is the fraction of correctly predicted tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c6e12e9291405eb0a84d84d2ae6fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval-Val', max=892, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.389707051585074, array([5.29454212, 0.20876536])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.evaluate('val', metrics=[ce, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   train_loss  val_loss    metrics     \n",
      "\n",
      "0       4.74847     4.43156     4.34480     0.27416       epoch run time: 21 min, 6.63 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-3\n",
    "learner.fit(lr,1,metrics=[ce, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   train_loss  val_loss    metrics     \n",
      "\n",
      "0       4.43827     4.21211     4.14686     0.29376       epoch run time: 22 min, 43.63 sec\n",
      "1       4.37775     4.13284     4.07652     0.30114       epoch run time: 22 min, 42.65 sec\n",
      "2       4.29097     4.08455     4.03664     0.30495       epoch run time: 22 min, 44.96 sec\n",
      "3       4.27232     4.06161     4.02322     0.30654       epoch run time: 22 min, 43.93 sec\n",
      "4       4.24789     4.04641     4.01397     0.30772       epoch run time: 22 min, 43.62 sec\n",
      "5       4.19398     4.02324     3.99348     0.31020       epoch run time: 22 min, 44.09 sec\n",
      "6       4.18061     4.00554     3.97723     0.31169       epoch run time: 22 min, 44.12 sec\n",
      "7       4.15095     3.98286     3.95542     0.31379       epoch run time: 22 min, 44.73 sec\n",
      "8       4.13074     3.96671     3.93895     0.31566       epoch run time: 22 min, 44.30 sec\n",
      "9       4.08665     3.95195     3.92297     0.31750       epoch run time: 22 min, 46.74 sec\n",
      "10      4.03330     3.94103     3.91015     0.31914       epoch run time: 22 min, 43.92 sec\n",
      "11      4.02811     3.93039     3.89826     0.32049       epoch run time: 22 min, 43.93 sec\n",
      "12      3.94755     3.92576     3.89195     0.32122       epoch run time: 22 min, 42.44 sec\n",
      "13      3.93758     3.92289     3.88825     0.32160       epoch run time: 22 min, 43.56 sec\n",
      "14      3.93712     3.92278     3.88786     0.32165       epoch run time: 22 min, 43.50 sec\n"
     ]
    }
   ],
   "source": [
    "learner.unfreeze()\n",
    "\n",
    "lr_max = [1e-3,5e-3]\n",
    "learner.fit_one_cycle(lr_max, 15, beta_min=0.8, beta_max=0.8, metrics=[ce,acc], \n",
    "                      save_name='lang_model_fwd', save_method='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ...\n",
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae9e0564f664e34bb05a222270b5614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68f5da6a5b349b79765c91b31892ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f0251b11ea4cdb9955cb673bbcd107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5047505e37c44bee8bf610f36e76c439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a99dfc7048412fb05d4431e0e4ece6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecaee17dd0b04f179243ba4774de1116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Numericalizing ...\n",
      "min freq = 6 , max_vocab = 60000\n",
      "Done, vocab_size =  47343\n"
     ]
    }
   ],
   "source": [
    "data = LanguageModelDataObj.from_folders(bs=64, bptt=75, labels=['pos','neg','unsup'], \n",
    "                                         train='IMDB/train', reverse=True)\n",
    "\n",
    "# save string-to-int dictionary from data\n",
    "pickle.dump(data.stoi, open('IMDB/data_stoi_bwd','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'IMDB'\n",
    "model = LanguageModelNet(data, pretrained='bwd')\n",
    "model.clear_non_raw()\n",
    "opt_func = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "optimizer = Optimizer(opt_func, model)\n",
    "loss_func = RegSeqCrossEntropyLoss()\n",
    "learner = Learner(PATH,data,model,optimizer,loss_func)\n",
    "learner.freeze()\n",
    "\n",
    "# metrics\n",
    "acc = LanguageModelAccuracy()\n",
    "ce = SeqCrossEntropyLoss(loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e304c573594e26bb05d89d8f5a5b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval-Val', max=893, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.454193824883404, array([5.36219153, 0.22140861])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.evaluate('val', metrics=[ce, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   train_loss  val_loss    metrics     \n",
      "\n",
      "0       4.77748     4.48984     4.40807     0.28296       epoch run time: 21 min, 4.89 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-3\n",
    "learner.fit(lr,1,metrics=[ce, acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   train_loss  val_loss    metrics     \n",
      "\n",
      "0       4.51578     4.27562     4.21574     0.30106       epoch run time: 22 min, 44.00 sec\n",
      "1       4.42960     4.18793     4.13646     0.30878       epoch run time: 22 min, 44.06 sec\n",
      "2       4.35127     4.13435     4.09161     0.31369       epoch run time: 22 min, 43.78 sec\n",
      "3       4.32624     4.09959     4.06321     0.31648       epoch run time: 22 min, 45.15 sec\n",
      "4       4.27456     4.07049     4.04048     0.31913       epoch run time: 22 min, 47.07 sec\n",
      "5       4.25433     4.04543     4.01872     0.32155       epoch run time: 22 min, 46.67 sec\n",
      "6       4.20313     4.01992     3.99372     0.32363       epoch run time: 22 min, 42.98 sec\n",
      "7       4.16608     4.00435     3.97848     0.32569       epoch run time: 22 min, 45.43 sec\n",
      "8       4.12145     3.98746     3.96094     0.32749       epoch run time: 22 min, 45.22 sec\n",
      "9       4.11600     3.97300     3.94531     0.32917       epoch run time: 22 min, 43.70 sec\n",
      "10      4.08827     3.96281     3.93368     0.33035       epoch run time: 22 min, 45.84 sec\n",
      "11      4.05154     3.95638     3.92562     0.33134       epoch run time: 22 min, 45.83 sec\n",
      "12      4.03049     3.95205     3.91988     0.33199       epoch run time: 22 min, 48.03 sec\n",
      "13      4.03391     3.94999     3.91704     0.33230       epoch run time: 22 min, 46.57 sec\n",
      "14      3.98571     3.95000     3.91678     0.33237       epoch run time: 22 min, 47.15 sec\n"
     ]
    }
   ],
   "source": [
    "learner.unfreeze()\n",
    "\n",
    "lr_max = [1e-3,5e-3]\n",
    "learner.fit_one_cycle(lr_max, 15, beta_min=0.8, beta_max=0.8, metrics=[ce,acc],\n",
    "                      save_name='lang_model_bwd', save_method='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (2) - Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "This Notebook was saved, shutdown, and reopened later at this point. So we need to do imports again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from Applications.Text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Forward Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ...\n",
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431d656117194c4286753fe4e6e9e936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7fdd696e9b4924b5d27c184909a64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2fa295705041d5ab2fd1a4836f341a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb87d896ccd4f0780c8569f67fc8506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396d25859ebe4290903ae1e88a14a93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d402e078cf64e4c9f69db45aa7af29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Numericalizing ...\n",
      "min freq = 6 , max_vocab = 60000\n",
      "Done, vocab_size =  47343\n"
     ]
    }
   ],
   "source": [
    "data = LanguageModelDataObj.from_folders(bs=64, bptt=75, labels=['pos','neg','unsup'], \n",
    "                                         train='IMDB/train', reverse=False)\n",
    "\n",
    "PATH = 'IMDB'\n",
    "model = LanguageModelNet(data, pretrained='fwd')\n",
    "model.clear_non_raw()\n",
    "loss_func = RegSeqCrossEntropyLoss()\n",
    "learner = Learner(PATH,data,model,loss_func=loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51076e99c3d749ab968521e40bb2605d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval-Val', max=884, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.388274078455446]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.evaluate('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d582ba26c2554d9883073ca3e57ac229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval-Val', max=884, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.649482626720791]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.load('lang_model_fwd_14')\n",
    "learner.evaluate('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = learner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Text Classification DataObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = pickle.load(open('IMDB/data_stoi_fwd','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ...\n",
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85299bdadd94b3b9171411a59bdb086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd1407b13a94e7291952b3681326e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2fbf6b5776459e94e824d6d7105bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1a6d0a488b4192b5aca6755bbc5fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985b0e5f15834f609ac4120238277dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0b118458cf4729a1b114e9e7863d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4165), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Numericalizing ...\n",
      "min freq = 6 , max_vocab = 60000\n",
      "Done, vocab_size =  47343\n"
     ]
    }
   ],
   "source": [
    "data = TextClassificationDataObj.from_folders(bs=64, labels=['pos','neg'], train='IMDB/train', \n",
    "                                              test='IMDB/test', reverse=False, stoi=stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train 5 text classifiers, each starting from pretrained language model. \n",
    "We will store predictions on test set for each and ensemble them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   train_loss  val_loss    metrics     \n",
      "\n",
      "0       0.33171     0.22379     0.19196     0.92640       epoch run time: 2 min, 13.93 sec\n",
      "1       0.31970     0.21417     0.18720     0.92880       epoch run time: 2 min, 14.56 sec\n",
      "2       0.29380     0.19714     0.17507     0.93520       epoch run time: 2 min, 13.61 sec\n",
      "3       0.24482     0.20004     0.18194     0.93060       epoch run time: 2 min, 13.63 sec\n",
      "4       0.23394     0.17767     0.16192     0.93860       epoch run time: 2 min, 14.32 sec\n",
      "5       0.21873     0.17981     0.16534     0.93840       epoch run time: 2 min, 14.76 sec\n",
      "6       0.20520     0.17550     0.16175     0.94120       epoch run time: 2 min, 15.10 sec\n",
      "7       0.18456     0.17531     0.16186     0.94240       epoch run time: 2 min, 15.33 sec\n",
      "8       0.18255     0.18150     0.16820     0.94200       epoch run time: 2 min, 14.29 sec\n",
      "9       0.17305     0.17584     0.16256     0.94260       epoch run time: 2 min, 13.19 sec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a377307305492d9870b3438d0ac32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Predicting', max=391, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = 'IMDB'\n",
    "loss_func = RegSeqCrossEntropyLoss()\n",
    "acc = TextClassificationAccuracy()\n",
    "ce = SeqCrossEntropyLoss(loss_func)\n",
    "PredProbsFwd, LossFwd, AccuracyFwd = [],[],[]\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print('training model ' + str(i))   \n",
    "    model = TextClassificationNet(PATH, language_model, num_classes=2, drop_scaling = 1.5)\n",
    "    model.clear_non_raw()\n",
    "    opt_func = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "    optimizer = Optimizer(opt_func, model)\n",
    "    learner = Learner(PATH,data,model,optimizer,loss_func)\n",
    "    \n",
    "    learner.freeze()\n",
    "    learner.fit(lr=1e-3, num_epochs=1, metrics=[ce,acc])\n",
    "    \n",
    "    learner.unfreeze()\n",
    "    learner.fit_one_cycle(lr_max=[2e-4,1e-3,5e-3], num_epochs=10, beta_min=0.7, beta_max=0.7, clip=1.0,\n",
    "                          metrics=[ce,acc], save_name='text_classifier_fwd_'+str(i), save_method='best')\n",
    "    \n",
    "    learner.load('text_classifier_fwd_'+str(i))\n",
    "    pred_probs, pred_labels = learner.predict('test')\n",
    "    labels = np.array(learner.data.test_ds.labels)\n",
    "    \n",
    "    PredProbsFwd.append(pred_probs)\n",
    "    LossFwd.append( skm.log_loss(labels,pred_probs) )\n",
    "    AccuracyFwd.append( skm.accuracy_score(labels,pred_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148289</td>\n",
       "      <td>0.94472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148750</td>\n",
       "      <td>0.94620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148021</td>\n",
       "      <td>0.94624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148969</td>\n",
       "      <td>0.94484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150276</td>\n",
       "      <td>0.94628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy\n",
       "0  0.148289   0.94472\n",
       "1  0.148750   0.94620\n",
       "2  0.148021   0.94624\n",
       "3  0.148969   0.94484\n",
       "4  0.150276   0.94628"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results = pd.DataFrame({'loss':LossFwd, 'accuracy':AccuracyFwd})\n",
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Backward Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ...\n",
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5bb743f0e74fa3b2f26dc103fa8ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f731afedf5a4322b4ccff27a2c91a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6294bc6fcd413184221393b1acee64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4833d9cd4a45a488e560196738c992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2142a4ed74b14107a21dadd0e2ea8b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d4bdc1282841ec9884661d54310c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Numericalizing ...\n",
      "min freq = 6 , max_vocab = 60000\n",
      "Done, vocab_size =  47343\n"
     ]
    }
   ],
   "source": [
    "data = LanguageModelDataObj.from_folders(bs=64, bptt=75, labels=['pos','neg','unsup'], \n",
    "                                         train='IMDB/train', reverse=True)\n",
    "\n",
    "PATH = 'IMDB'\n",
    "model = LanguageModelNet(data, pretrained='bwd')\n",
    "model.clear_non_raw()\n",
    "loss_func = RegSeqCrossEntropyLoss()\n",
    "learner = Learner(PATH,data,model,loss_func=loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9f28ba60db4e38acd983ba690765b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval-Val', max=889, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.454925678712311]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.evaluate('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b1bc04f42643ecab4557c3d4d6fb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval-Val', max=889, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.7176763767332544]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.load('lang_model_bwd_14')\n",
    "learner.evaluate('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = learner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Text Classification DataObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = pickle.load(open('IMDB/data_stoi_bwd','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ...\n",
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a244b5071947a78e616b4d773fac75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85def5dc07a487998672da3b4b3894e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1339e7c9a54016a9724b5b6e7987a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cedf349e18464aa366a6d32129c4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964b67937b914ec0acbc35a7510ba092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729e7d8d7bc441b8a6620578a6dcc489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4165), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Numericalizing ...\n",
      "min freq = 6 , max_vocab = 60000\n",
      "Done, vocab_size =  47343\n"
     ]
    }
   ],
   "source": [
    "data = TextClassificationDataObj.from_folders(bs=64, labels=['pos','neg'], train='IMDB/train', \n",
    "                                              val=None, test='IMDB/test', reverse=True, stoi=stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train 5 text classifiers, each starting from pretrained language model. \n",
    "We will store predictions on test set for each and ensemble them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   train_loss  val_loss    metrics     \n",
      "\n",
      "0       0.29904     0.21705     0.18665     0.92640       epoch run time: 2 min, 14.77 sec\n",
      "1       0.27228     0.20736     0.18059     0.93040       epoch run time: 2 min, 14.23 sec\n",
      "2       0.25611     0.19646     0.17434     0.93100       epoch run time: 2 min, 14.43 sec\n",
      "3       0.22424     0.20495     0.18600     0.93480       epoch run time: 2 min, 13.74 sec\n",
      "4       0.21011     0.18275     0.16595     0.93520       epoch run time: 2 min, 14.49 sec\n",
      "5       0.18539     0.18172     0.16626     0.93760       epoch run time: 2 min, 14.29 sec\n",
      "6       0.17514     0.18317     0.16838     0.93820       epoch run time: 2 min, 14.91 sec\n",
      "7       0.16173     0.17566     0.16123     0.94080       epoch run time: 2 min, 14.40 sec\n",
      "8       0.13536     0.18572     0.17138     0.94060       epoch run time: 2 min, 14.06 sec\n",
      "9       0.13861     0.18716     0.17284     0.93660       epoch run time: 2 min, 14.41 sec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4309a4f0343248f6a43e9d8fd0e88572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Predicting', max=391, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = 'IMDB'\n",
    "loss_func = RegSeqCrossEntropyLoss()\n",
    "acc = TextClassificationAccuracy()\n",
    "ce = SeqCrossEntropyLoss(loss_func)\n",
    "PredProbsBwd, LossBwd, AccuracyBwd = [],[],[]\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print('training model ' + str(i))   \n",
    "    model = TextClassificationNet(PATH, language_model, num_classes=2, drop_scaling = 1.0)\n",
    "    model.clear_non_raw()\n",
    "    opt_func = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "    optimizer = Optimizer(opt_func, model)\n",
    "    learner = Learner(PATH,data,model,optimizer,loss_func)\n",
    "    \n",
    "    learner.freeze()\n",
    "    learner.fit(lr=1e-3, num_epochs=1, metrics=[ce,acc])\n",
    "    \n",
    "    learner.unfreeze()\n",
    "    learner.fit_one_cycle(lr_max=[2e-4,1e-3,5e-3], num_epochs=10, beta_min=0.7, beta_max=0.7, clip=1.0, \n",
    "                          metrics=[ce,acc], save_name='text_classifier_bwd_'+str(i), save_method='best')\n",
    "    \n",
    "    learner.load('text_classifier_bwd_'+str(i))\n",
    "    pred_probs, pred_labels = learner.predict('test')\n",
    "    labels = np.array(learner.data.test_ds.labels)\n",
    "    \n",
    "    PredProbsBwd.append(pred_probs)\n",
    "    LossBwd.append( skm.log_loss(labels,pred_probs) )\n",
    "    AccuracyBwd.append( skm.accuracy_score(labels,pred_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.154592</td>\n",
       "      <td>0.94228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156131</td>\n",
       "      <td>0.94232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.181118</td>\n",
       "      <td>0.93044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.155513</td>\n",
       "      <td>0.94216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.154111</td>\n",
       "      <td>0.94248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy\n",
       "0  0.154592   0.94228\n",
       "1  0.156131   0.94232\n",
       "2  0.181118   0.93044\n",
       "3  0.155513   0.94216\n",
       "4  0.154111   0.94248"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results = pd.DataFrame({'loss':LossBwd, 'accuracy':AccuracyBwd})\n",
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging All Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1371316749236456\n",
      "accuracy: 0.94968\n"
     ]
    }
   ],
   "source": [
    "predprobs_fwd_avg = np.array(PredProbsFwd).mean(axis=0)\n",
    "predprobs_bwd_avg = np.array(PredProbsBwd).mean(axis=0)\n",
    "predprobs_avg = (predprobs_fwd_avg + predprobs_bwd_avg)/2\n",
    "predlabels_avg = np.argmax(predprobs_avg,axis=1)\n",
    "\n",
    "loss = skm.log_loss(labels,predprobs_avg)\n",
    "accuracy = skm.accuracy_score(labels,predlabels_avg)\n",
    "print('loss:',loss)\n",
    "print('accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging Only Predictions for Forward Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.14445905760379318\n",
      "accuracy: 0.94656\n"
     ]
    }
   ],
   "source": [
    "predlabels_fwd = np.argmax(predprobs_fwd_avg,axis=1)\n",
    "loss = skm.log_loss(labels,predprobs_fwd_avg)\n",
    "accuracy = skm.accuracy_score(labels,predlabels_fwd)\n",
    "print('loss:',loss)\n",
    "print('accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging Only Predictions for Backward Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1517001601814566\n",
      "accuracy: 0.94372\n"
     ]
    }
   ],
   "source": [
    "predlabels_bwd = np.argmax(predprobs_bwd_avg,axis=1)\n",
    "loss = skm.log_loss(labels,predprobs_bwd_avg)\n",
    "accuracy = skm.accuracy_score(labels,predlabels_bwd)\n",
    "print('loss:',loss)\n",
    "print('accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there is a definite improvement from using both forward text and backward text predictions in the ensemble. \n",
    "\n",
    "To improve further, we will try 1 small tweak. Trial number 2 of training for the backward texts had a significantly worse accuracy than the others (about 0.930 vs about 0.942 for all others). I'm not sure why the model did not train so well that time, but we will try omitting the predictions of this trial in our backward ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.13573877105064341\n",
      "accuracy: 0.9504\n"
     ]
    }
   ],
   "source": [
    "predprobs_fwd_avg = np.array(PredProbsFwd).mean(axis=0)\n",
    "predprobs_bwd_avg = np.array(PredProbsBwd[:2] + PredProbsBwd[3:]).mean(axis=0)\n",
    "predprobs_avg = (predprobs_fwd_avg + predprobs_bwd_avg)/2\n",
    "predlabels_avg = np.argmax(predprobs_avg,axis=1)\n",
    "\n",
    "loss = skm.log_loss(labels,predprobs_avg)\n",
    "accuracy = skm.accuracy_score(labels,predlabels_avg)\n",
    "print('loss:',loss)\n",
    "print('accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, yes, omitting the \"bad\" trial number 2 with the backward texts did lead to a small further improvement.\n",
    "\n",
    "Final Accuracy = 0.9504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
